"""
å¼·åŒ–ã•ã‚ŒãŸDocument Intelligence ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 

å¾“é‡èª²é‡‘ã®æœ€é©åŒ–ã®ãŸã‚ã€PDFãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã¨ãƒšãƒ¼ã‚¸å˜ä½ã®
ä¸¡æ–¹ã®ãƒ¬ãƒ™ãƒ«ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç®¡ç†ã™ã‚‹æ´—ç·´ã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ã€‚
"""

import os
import json
import hashlib
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum


class CacheLevel(Enum):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ãƒ™ãƒ«"""
    FULL_DOCUMENT = "full_document"  # PDFå…¨ä½“
    INDIVIDUAL_PAGE = "individual_page"  # å€‹åˆ¥ãƒšãƒ¼ã‚¸


@dataclass
class CacheMetadata:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    file_hash: str
    cache_type: CacheLevel
    original_filename: str
    page_number: Optional[int] = None
    parent_document_hash: Optional[str] = None
    file_size: int = 0
    processing_time: float = 0.0
    created_at: str = ""
    last_accessed: str = ""
    access_count: int = 0
    content_length: int = 0


class EnhancedDocumentIntelligenceCache:
    """
    å¼·åŒ–ã•ã‚ŒãŸDocument Intelligence ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
    
    ç‰¹å¾´:
    - éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆPDFå…¨ä½“ + å€‹åˆ¥ãƒšãƒ¼ã‚¸ï¼‰
    - SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚ˆã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†
    - ã‚³ã‚¹ãƒˆæœ€é©åŒ–æ©Ÿèƒ½
    - ã‚¢ã‚¯ã‚»ã‚¹çµ±è¨ˆã¨ãƒ¬ãƒãƒ¼ãƒˆ
    """
    
    def __init__(self, cache_dir: str = "enhanced_di_cache"):
        """
        å¼·åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        
        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.full_doc_cache_dir = self.cache_dir / "full_documents"
        self.page_cache_dir = self.cache_dir / "pages"
        self.full_doc_cache_dir.mkdir(exist_ok=True)
        self.page_cache_dir.mkdir(exist_ok=True)
        
        # SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self.db_path = self.cache_dir / "cache_metadata.db"
        self._init_database()
        
        # çµ±è¨ˆæƒ…å ±
        self.session_stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "bytes_saved": 0,
            "api_calls_saved": 0
        }
    
    def _init_database(self):
        """SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS cache_metadata (
                    file_hash TEXT PRIMARY KEY,
                    cache_type TEXT NOT NULL,
                    original_filename TEXT NOT NULL,
                    page_number INTEGER,
                    parent_document_hash TEXT,
                    file_size INTEGER DEFAULT 0,
                    processing_time REAL DEFAULT 0.0,
                    created_at TEXT NOT NULL,
                    last_accessed TEXT NOT NULL,
                    access_count INTEGER DEFAULT 0,
                    content_length INTEGER DEFAULT 0
                )
            ''')
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            conn.execute('CREATE INDEX IF NOT EXISTS idx_cache_type ON cache_metadata(cache_type)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_parent_hash ON cache_metadata(parent_document_hash)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_filename ON cache_metadata(original_filename)')
            
            conn.commit()
    
    def _get_file_hash(self, file_bytes: bytes, prefix: str = "") -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãï¼‰
        
        Args:
            file_bytes: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            prefix: ãƒãƒƒã‚·ãƒ¥ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆãƒšãƒ¼ã‚¸ç•ªå·ãªã©ï¼‰
            
        Returns:
            ãƒãƒƒã‚·ãƒ¥å€¤
        """
        hasher = hashlib.sha256()
        if prefix:
            hasher.update(prefix.encode('utf-8'))
        hasher.update(file_bytes)
        return hasher.hexdigest()
    
    def _get_cache_file_path(self, file_hash: str, cache_level: CacheLevel) -> Path:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—"""
        if cache_level == CacheLevel.FULL_DOCUMENT:
            return self.full_doc_cache_dir / f"{file_hash}.json"
        else:
            return self.page_cache_dir / f"{file_hash}.json"
    
    def _save_cache_metadata(self, metadata: CacheMetadata):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                INSERT OR REPLACE INTO cache_metadata 
                (file_hash, cache_type, original_filename, page_number, parent_document_hash,
                 file_size, processing_time, created_at, last_accessed, access_count, content_length)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                metadata.file_hash,
                metadata.cache_type.value,
                metadata.original_filename,
                metadata.page_number,
                metadata.parent_document_hash,
                metadata.file_size,
                metadata.processing_time,
                metadata.created_at,
                metadata.last_accessed,
                metadata.access_count,
                metadata.content_length
            ))
            conn.commit()
    
    def _get_cache_metadata(self, file_hash: str) -> Optional[CacheMetadata]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                'SELECT * FROM cache_metadata WHERE file_hash = ?',
                (file_hash,)
            )
            row = cursor.fetchone()
            
            if row:
                return CacheMetadata(
                    file_hash=row[0],
                    cache_type=CacheLevel(row[1]),
                    original_filename=row[2],
                    page_number=row[3],
                    parent_document_hash=row[4],
                    file_size=row[5],
                    processing_time=row[6],
                    created_at=row[7],
                    last_accessed=row[8],
                    access_count=row[9],
                    content_length=row[10]
                )
        return None
    
    def _update_access_info(self, file_hash: str):
        """ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±ã‚’æ›´æ–°"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                UPDATE cache_metadata 
                SET last_accessed = ?, access_count = access_count + 1
                WHERE file_hash = ?
            ''', (datetime.now().isoformat(), file_hash))
            conn.commit()
    
    def has_full_document_cache(self, file_bytes: bytes, filename: str) -> bool:
        """PDFå…¨ä½“ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        file_hash = self._get_file_hash(file_bytes)
        cache_file = self._get_cache_file_path(file_hash, CacheLevel.FULL_DOCUMENT)
        metadata = self._get_cache_metadata(file_hash)
        
        return cache_file.exists() and metadata is not None
    
    def has_page_cache(self, page_bytes: bytes, filename: str, page_number: int, parent_hash: str) -> bool:
        """å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        page_hash = self._get_file_hash(page_bytes, f"page_{page_number}")
        cache_file = self._get_cache_file_path(page_hash, CacheLevel.INDIVIDUAL_PAGE)
        metadata = self._get_cache_metadata(page_hash)
        
        return cache_file.exists() and metadata is not None
    
    def get_full_document_cache(self, file_bytes: bytes, filename: str) -> Optional[List[Dict[str, Any]]]:
        """PDFå…¨ä½“ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å–å¾—"""
        file_hash = self._get_file_hash(file_bytes)
        
        if not self.has_full_document_cache(file_bytes, filename):
            self.session_stats["cache_misses"] += 1
            return None
        
        try:
            cache_file = self._get_cache_file_path(file_hash, CacheLevel.FULL_DOCUMENT)
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            
            # ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±æ›´æ–°
            self._update_access_info(file_hash)
            self.session_stats["cache_hits"] += 1
            self.session_stats["api_calls_saved"] += 1
            
            print(f"ğŸ“‹ å…¨æ–‡æ›¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {filename}")
            return cached_data["pages_content"]
            
        except Exception as e:
            print(f"âŒ å…¨æ–‡æ›¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.session_stats["cache_misses"] += 1
            return None
    
    def get_page_cache(self, page_bytes: bytes, filename: str, page_number: int, parent_hash: str) -> Optional[Dict[str, Any]]:
        """å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å–å¾—"""
        page_hash = self._get_file_hash(page_bytes, f"page_{page_number}")
        
        if not self.has_page_cache(page_bytes, filename, page_number, parent_hash):
            self.session_stats["cache_misses"] += 1
            return None
        
        try:
            cache_file = self._get_cache_file_path(page_hash, CacheLevel.INDIVIDUAL_PAGE)
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            
            # ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±æ›´æ–°
            self._update_access_info(page_hash)
            self.session_stats["cache_hits"] += 1
            self.session_stats["api_calls_saved"] += 1
            
            print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {filename} ãƒšãƒ¼ã‚¸{page_number}")
            return cached_data["page_content"]
            
        except Exception as e:
            print(f"âŒ ãƒšãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.session_stats["cache_misses"] += 1
            return None
    
    def save_full_document_cache(self, file_bytes: bytes, filename: str, pages_content: List[Dict[str, Any]], processing_time: float = 0.0) -> bool:
        """PDFå…¨ä½“ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        file_hash = self._get_file_hash(file_bytes)
        
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
            cache_data = {
                "file_hash": file_hash,
                "filename": filename,
                "cached_at": datetime.now().isoformat(),
                "cache_type": "full_document",
                "pages_content": pages_content
            }
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            cache_file = self._get_cache_file_path(file_hash, CacheLevel.FULL_DOCUMENT)
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            metadata = CacheMetadata(
                file_hash=file_hash,
                cache_type=CacheLevel.FULL_DOCUMENT,
                original_filename=filename,
                file_size=len(file_bytes),
                processing_time=processing_time,
                created_at=datetime.now().isoformat(),
                last_accessed=datetime.now().isoformat(),
                access_count=0,
                content_length=len(json.dumps(pages_content))
            )
            self._save_cache_metadata(metadata)
            
            print(f"ğŸ’¾ å…¨æ–‡æ›¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {filename}")
            return True
            
        except Exception as e:
            print(f"âŒ å…¨æ–‡æ›¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def save_page_cache(self, page_bytes: bytes, filename: str, page_number: int, parent_hash: str, page_content: Dict[str, Any], processing_time: float = 0.0) -> bool:
        """å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
        page_hash = self._get_file_hash(page_bytes, f"page_{page_number}")
        
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
            cache_data = {
                "page_hash": page_hash,
                "filename": filename,
                "page_number": page_number,
                "parent_document_hash": parent_hash,
                "cached_at": datetime.now().isoformat(),
                "cache_type": "individual_page",
                "page_content": page_content
            }
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            cache_file = self._get_cache_file_path(page_hash, CacheLevel.INDIVIDUAL_PAGE)
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            metadata = CacheMetadata(
                file_hash=page_hash,
                cache_type=CacheLevel.INDIVIDUAL_PAGE,
                original_filename=filename,
                page_number=page_number,
                parent_document_hash=parent_hash,
                file_size=len(page_bytes),
                processing_time=processing_time,
                created_at=datetime.now().isoformat(),
                last_accessed=datetime.now().isoformat(),
                access_count=0,
                content_length=len(json.dumps(page_content))
            )
            self._save_cache_metadata(metadata)
            
            print(f"ğŸ’¾ ãƒšãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {filename} ãƒšãƒ¼ã‚¸{page_number}")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒšãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_cache_recommendations(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ã®æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        with sqlite3.connect(self.db_path) as conn:
            # å…¨ä½“çµ±è¨ˆ
            total_count = conn.execute('SELECT COUNT(*) FROM cache_metadata').fetchone()[0]
            
            # ã‚¢ã‚¯ã‚»ã‚¹é »åº¦åˆ†æ
            low_access = conn.execute(
                'SELECT COUNT(*) FROM cache_metadata WHERE access_count <= 1'
            ).fetchone()[0]
            
            # ã‚µã‚¤ã‚ºåˆ†æ
            large_files = conn.execute(
                'SELECT COUNT(*) FROM cache_metadata WHERE file_size > 5242880'  # 5MB
            ).fetchone()[0]
            
            # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«
            cutoff_date = (datetime.now() - timedelta(days=30)).isoformat()
            old_files = conn.execute(
                'SELECT COUNT(*) FROM cache_metadata WHERE last_accessed < ?',
                (cutoff_date,)
            ).fetchone()[0]
        
        recommendations = []
        
        if low_access > total_count * 0.3:
            recommendations.append("ä½ã‚¢ã‚¯ã‚»ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤šã„ã§ã™ã€‚å®šæœŸçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        if large_files > 0:
            recommendations.append(f"{large_files}å€‹ã®å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ã¾ã™ã€‚")
        
        if old_files > 0:
            recommendations.append(f"{old_files}å€‹ã®å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        return {
            "total_cached_items": total_count,
            "low_access_items": low_access,
            "large_files": large_files,
            "old_files": old_files,
            "recommendations": recommendations,
            "session_stats": self.session_stats
        }
    
    def cleanup_by_criteria(self, days_old: int = 30, min_access_count: int = 1, max_size_mb: float = None) -> int:
        """æ¡ä»¶ã«åŸºã¥ãã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        removed_count = 0
        cutoff_date = (datetime.now() - timedelta(days=days_old)).isoformat()
        
        with sqlite3.connect(self.db_path) as conn:
            # å‰Šé™¤å¯¾è±¡ã‚’ç‰¹å®š
            query = '''
                SELECT file_hash, cache_type FROM cache_metadata 
                WHERE (last_accessed < ? OR access_count < ?)
            '''
            params = [cutoff_date, min_access_count]
            
            if max_size_mb:
                query += ' OR file_size > ?'
                params.append(max_size_mb * 1024 * 1024)
            
            cursor = conn.execute(query, params)
            items_to_remove = cursor.fetchall()
            
            for file_hash, cache_type in items_to_remove:
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                    cache_level = CacheLevel(cache_type)
                    cache_file = self._get_cache_file_path(file_hash, cache_level)
                    if cache_file.exists():
                        cache_file.unlink()
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
                    conn.execute('DELETE FROM cache_metadata WHERE file_hash = ?', (file_hash,))
                    removed_count += 1
                    
                except Exception as e:
                    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ã‚¤ãƒ†ãƒ å‰Šé™¤ã‚¨ãƒ©ãƒ¼ {file_hash}: {e}")
            
            conn.commit()
        
        print(f"ğŸ§¹ {removed_count}å€‹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        return removed_count
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªçµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            # åŸºæœ¬çµ±è¨ˆ
            stats = {}
            
            # ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
            cursor = conn.execute('''
                SELECT cache_type, COUNT(*), SUM(file_size), AVG(processing_time), SUM(access_count)
                FROM cache_metadata GROUP BY cache_type
            ''')
            
            for cache_type, count, total_size, avg_time, total_access in cursor:
                stats[cache_type] = {
                    "count": count,
                    "total_size_mb": round((total_size or 0) / (1024 * 1024), 2),
                    "avg_processing_time": round(avg_time or 0, 2),
                    "total_accesses": total_access or 0
                }
            
            # å…¨ä½“çµ±è¨ˆ
            cursor = conn.execute('''
                SELECT COUNT(*), SUM(file_size), AVG(processing_time), SUM(access_count)
                FROM cache_metadata
            ''')
            total_count, total_size, avg_time, total_access = cursor.fetchone()
            
            stats["overall"] = {
                "total_items": total_count or 0,
                "total_size_mb": round((total_size or 0) / (1024 * 1024), 2),
                "avg_processing_time": round(avg_time or 0, 2),
                "total_accesses": total_access or 0,
                "cache_directory": str(self.cache_dir)
            }
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆè¿½åŠ 
            stats["session"] = self.session_stats
            
        return stats