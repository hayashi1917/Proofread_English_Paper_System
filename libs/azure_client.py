from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
from langchain_openai import ChatOpenAI
import openai
import os
from typing import List, Dict, Any
from langchain.tools import BaseTool
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from app.services.knowledge.utils.document_intelligence_cache import DocumentIntelligenceCache
from app.services.knowledge.utils.enhanced_cache_system import EnhancedDocumentIntelligenceCache

load_dotenv(".env.local")

class AzureDocumentIntelligenceClient:
    def __init__(self, enable_cache: bool = True, use_enhanced_cache: bool = True) -> None:
        self.client = DocumentIntelligenceClient(
            endpoint=os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT"),
            credential=AzureKeyCredential(os.getenv("AZURE_DOCUMENT_INTELLIGENCE_KEY")),
        )
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        if enable_cache:
            if use_enhanced_cache:
                self.cache = EnhancedDocumentIntelligenceCache()
                self.cache_type = "enhanced"
            else:
                self.cache = DocumentIntelligenceCache()
                self.cache_type = "legacy"
        else:
            self.cache = None
            self.cache_type = "disabled"
        
    def analyze_document_client(self) -> DocumentIntelligenceClient:
        return self.client
    
    def analyze_pdf_pages(self, file_bytes: bytes, file_name: str) -> List[Dict[str, Any]]:
        """
        PDFã‚’ãƒšãƒ¼ã‚¸ã”ã¨ã«åˆ†æã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ããƒ»çœŸã®ãƒšãƒ¼ã‚¸åˆ†å‰²å¯¾å¿œï¼‰
        
        Args:
            file_bytes: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå‚ç…§ç”¨ï¼‰
            
        Returns:
            List[Dict]: ãƒšãƒ¼ã‚¸ã”ã¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        from app.services.knowledge.utils.pdf_page_splitter import PDFPageSplitter
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã§ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯å–å¾—
        if self.cache and self.cache.has_cache(file_bytes, file_name):
            cached_result = self.cache.get_cache(file_bytes, file_name)
            if cached_result is not None:
                return cached_result
        
        print(f"Document Intelligence ã§è¤‡æ•°ãƒšãƒ¼ã‚¸å‡¦ç†ä¸­: {file_name}")
        
        # PDFã‚’ç‰©ç†çš„ã«ãƒšãƒ¼ã‚¸ã”ã¨ã«åˆ†å‰²
        splitter = PDFPageSplitter()
        pdf_info = splitter.get_pdf_info(file_bytes)
        print(f"PDFæƒ…å ±: {pdf_info['page_count']} ãƒšãƒ¼ã‚¸ - {file_name}")
        
        # PDFã‚’å€‹åˆ¥ãƒšãƒ¼ã‚¸ã«åˆ†å‰²
        pages_data = splitter.split_pdf_to_pages(file_bytes, file_name)
        
        # å„ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã« Document Intelligence ã§å‡¦ç†
        pages_content = []
        for page_data in pages_data:
            page_number = page_data["page_number"]
            page_bytes = page_data["page_bytes"]
            page_file_name = page_data["page_file_name"]
            
            print(f"ãƒšãƒ¼ã‚¸ {page_number} ã‚’ Document Intelligence ã§å‡¦ç†ä¸­...")
            
            try:
                # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚’ Document Intelligence ã§åˆ†æ
                poller = self.client.begin_analyze_document(
                    model_id="prebuilt-read",
                    body=page_bytes,
                    output_content_format="markdown",
                )
                result = poller.result()
                
                # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’æŠ½å‡º
                page_content = ""
                if hasattr(result, 'content') and result.content:
                    page_content = result.content.strip()
                elif hasattr(result, 'pages') and result.pages:
                    # fallback: ãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æŠ½å‡º
                    for page in result.pages:
                        if hasattr(page, 'lines') and page.lines:
                            for line in page.lines:
                                if hasattr(line, 'content'):
                                    page_content += line.content + "\n"
                    page_content = page_content.strip()
                
                # ãƒšãƒ¼ã‚¸ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
                if page_content:
                    pages_content.append({
                        "page_number": page_number,
                        "content": page_content,
                        "source_file": file_name,
                        "page_file_name": page_file_name
                    })
                    print(f"ãƒšãƒ¼ã‚¸ {page_number} ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ ({len(page_content)} æ–‡å­—)")
                else:
                    print(f"ãƒšãƒ¼ã‚¸ {page_number} ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    
            except Exception as e:
                print(f"ãƒšãƒ¼ã‚¸ {page_number} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
        
        print(f"åˆè¨ˆ {len(pages_content)} ãƒšãƒ¼ã‚¸ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        
        # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if self.cache:
            self.cache.save_cache(file_bytes, file_name, pages_content)
        
        return pages_content
    
    def analyze_pdf_pages_with_enhanced_cache(self, file_bytes: bytes, file_name: str) -> List[Dict[str, Any]]:
        """
        å¼·åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦PDFã‚’ãƒšãƒ¼ã‚¸ã”ã¨ã«åˆ†æï¼ˆã‚³ã‚¹ãƒˆæœ€é©åŒ–ç‰ˆï¼‰
        
        Args:
            file_bytes: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå‚ç…§ç”¨ï¼‰
            
        Returns:
            List[Dict]: ãƒšãƒ¼ã‚¸ã”ã¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        if not self.cache or self.cache_type != "enhanced":
            # å¼·åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯é€šå¸¸å‡¦ç†
            return self.analyze_pdf_pages(file_bytes, file_name)
        
        from app.services.knowledge.utils.pdf_page_splitter import PDFPageSplitter
        import time
        
        print(f"ğŸ’ å¼·åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã§PDFå‡¦ç†é–‹å§‹: {file_name}")
        
        # ã¾ãšå…¨æ–‡æ›¸ãƒ¬ãƒ™ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
        full_doc_cached_result = self.cache.get_full_document_cache(file_bytes, file_name)
        if full_doc_cached_result is not None:
            print(f"ğŸ¯ å…¨æ–‡æ›¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {file_name}")
            return full_doc_cached_result
        
        # å…¨æ–‡æ›¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã€ãƒšãƒ¼ã‚¸åˆ†å‰²ã—ã¦å€‹åˆ¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
        splitter = PDFPageSplitter()
        pdf_info = splitter.get_pdf_info(file_bytes)
        print(f"PDFæƒ…å ±: {pdf_info['page_count']} ãƒšãƒ¼ã‚¸ - {file_name}")
        
        # PDFã‚’å€‹åˆ¥ãƒšãƒ¼ã‚¸ã«åˆ†å‰²
        pages_data = splitter.split_pdf_to_pages(file_bytes, file_name)
        parent_hash = self.cache._get_file_hash(file_bytes)
        
        # å„ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ + å¿…è¦ã«å¿œã˜ã¦APIå‘¼ã³å‡ºã—ï¼‰
        pages_content = []
        total_processing_time = 0.0
        
        for page_data in pages_data:
            page_number = page_data["page_number"]
            page_bytes = page_data["page_bytes"]
            page_file_name = page_data["page_file_name"]
            
            start_time = time.time()
            
            # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
            cached_page_content = self.cache.get_page_cache(
                page_bytes, file_name, page_number, parent_hash
            )
            
            if cached_page_content is not None:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
                pages_content.append(cached_page_content)
                continue
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: Document Intelligence APIå‘¼ã³å‡ºã—
            print(f"ğŸ”„ ãƒšãƒ¼ã‚¸ {page_number} ã‚’ Document Intelligence ã§å‡¦ç†ä¸­...")
            
            try:
                # Document Intelligence ã§å‡¦ç†
                poller = self.client.begin_analyze_document(
                    model_id="prebuilt-read",
                    body=page_bytes,
                    output_content_format="markdown",
                )
                result = poller.result()
                
                # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’æŠ½å‡º
                page_content = ""
                if hasattr(result, 'content') and result.content:
                    page_content = result.content.strip()
                elif hasattr(result, 'pages') and result.pages:
                    for page in result.pages:
                        if hasattr(page, 'lines') and page.lines:
                            for line in page.lines:
                                if hasattr(line, 'content'):
                                    page_content += line.content + "\n"
                    page_content = page_content.strip()
                
                processing_time = time.time() - start_time
                total_processing_time += processing_time
                
                if page_content:
                    page_result = {
                        "page_number": page_number,
                        "content": page_content,
                        "source_file": file_name,
                        "page_file_name": page_file_name
                    }
                    
                    pages_content.append(page_result)
                    
                    # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                    self.cache.save_page_cache(
                        page_bytes, file_name, page_number, parent_hash,
                        page_result, processing_time
                    )
                    
                    print(f"âœ… ãƒšãƒ¼ã‚¸ {page_number} å‡¦ç†å®Œäº† ({processing_time:.2f}ç§’)")
                else:
                    print(f"âš ï¸ ãƒšãƒ¼ã‚¸ {page_number} ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    
            except Exception as e:
                print(f"âŒ ãƒšãƒ¼ã‚¸ {page_number} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"ğŸ“Š å‡¦ç†å®Œäº†: {len(pages_content)} ãƒšãƒ¼ã‚¸ (ç·å‡¦ç†æ™‚é–“: {total_processing_time:.2f}ç§’)")
        
        # å…¨æ–‡æ›¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ä¿å­˜ï¼ˆå°†æ¥ã®é«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
        if pages_content:
            self.cache.save_full_document_cache(
                file_bytes, file_name, pages_content, total_processing_time
            )
        
        return pages_content
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çµ±è¨ˆæƒ…å ±ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åŠ¹ã®å ´åˆã¯None
        """
        if self.cache:
            if self.cache_type == "enhanced":
                stats = self.cache.get_comprehensive_stats()
                stats["cache_type"] = "enhanced"
                return stats
            else:
                stats = self.cache.get_cache_stats()
                stats["cache_type"] = "legacy"
                return stats
        return {"cache_enabled": False, "cache_type": "disabled"}
    
    def list_cached_files(self) -> List[Dict[str, Any]]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã‚’å–å¾—
        
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ãƒªã‚¹ãƒˆã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åŠ¹ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆ
        """
        if self.cache:
            return self.cache.list_cached_files()
        return []
    
    def cleanup_old_cache(self, days: int = 30):
        """
        å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        
        Args:
            days: ä¿æŒæœŸé–“ï¼ˆæ—¥æ•°ï¼‰
        """
        if self.cache:
            self.cache.cleanup_old_cache(days)      

class AzureOpenAIClient:
    def __init__(self,tools: List[BaseTool] | None = None):
        # â˜… 1) max_completion_tokens ã§â€œæ›¸ãéãâ€ãƒ–ãƒ­ãƒƒã‚¯
        # â˜… 2) tools / tool_choice ã‚’ãã®å ´ã§ bind ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
        self.client = ChatOpenAI(
            api_key=os.getenv("AZURE_OPENAI_KEY"),
            model_name="gpt-4.1-mini",
            temperature=0,
            max_completion_tokens=4096,
        )
        if tools:                          # tools ãŒæ¸¡ã•ã‚Œã¦ã„ãŸã‚‰å³ bind
            self.client = self.client.bind(
                tools=tools,
                tool_choice="auto"
            )

    def get_openai_client(self) -> ChatOpenAI:
        return self.client

class AzureOpenAIEmbeddings:
    def __init__(self):
        self.client = OpenAIEmbeddings(
            api_key=os.getenv("AZURE_OPENAI_KEY"),
            model="text-embedding-ada-002",
        )

    def get_openai_embeddings(self) -> OpenAIEmbeddings:
        return self.client


class ChromaDBClient:
    def __init__(self):
        azure_embeddings_client = AzureOpenAIEmbeddings()
        self.client = Chroma(
            collection_name="knowledge_base_db",
            persist_directory="chroma_db",
            embedding_function=azure_embeddings_client.get_openai_embeddings(),
        )

    def get_chroma_client(self) -> Chroma:
        return self.client
